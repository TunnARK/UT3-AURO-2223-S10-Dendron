---
id: 2rcssepj4fphwhot5h6l2wi
title: CM
desc: ''
updated: 1674060450412
created: 1674053496272
---

> **Avertissement:**
Cette page peut contenir des fautes ! Envoyez-moi un message sur [`#UT3-AURO-M2-2223-Request:matrix.org`](https://matrix.to/#/#UT3-AURO-M2-2223-Request:matrix.org) si vous en trouvez, merci.

> Cours donné par P. Muller, T. Pellegrini

---

> Notes RKA du 2023/01/18 - Start



![](/assets/images/B3.AA.CM.Slide-01.png)

# Modalités

![](/assets/images/B3.AA.CM.Slide-02.png)

# Objectifs généraux

![](/assets/images/B3.AA.CM.Slide-03.png)

Si pas assez de données plustôt utiliser des graphes ou des mélanges de gaussiennes (AI n'est pas toujours le plus optimale).

# Bibiliographie

![](/assets/images/B3.AA.CM.Slide-04.png)

# Aperçu

> Keywords:
- MLP - Multi-Layer Perceptron
- CNN - Connbolutional Neural Networks
- RNN - Recurrent Neural Networks
- seq2seq

![](/assets/images/B3.AA.CM.Slide-05.png)

Perceptron est la base d'un réseau de neuronne.

On peut combiner un CNN avec un RNN.

Les modèles séquences à séquences peuvent ne pas être non récurrent.
Les transformeurs sont des MLP sans récurrences.

Limites:
- Interpratibilité: peut-on interpréter le resultat final
- Biais: si les données fournis sont biaisées le resultat sera biaisées aussi donc il faut mettre en place des corrections pour les eviter si possible
- Exemple adversaires: données un modèle erroné pour tester l'algo ???

# ???

![](/assets/images/B3.AA.CM.Slide-06.png)

## Qu'est-ce que l'apprentissage par Machine Learning ?

![](/assets/images/B3.AA.CM.Slide-07.png)

### Définition

![](/assets/images/B3.AA.CM.Slide-08.png)

### Terminologie

![](/assets/images/B3.AA.CM.Slide-09.png)

Exemple: trier des e-mails
- Données = titres + texte d'email
    - correspondent aux **observations**
- Labels = Binaire --> Spam/Non-Spam
    - y = +1 pour Spam
    - y = 0  pour Non-Spam (**dans un perceptron on mettre à -1**)
- Comment encoder le texte ?
    - on cherche à travailler avec des réelles (il existe des algo binaires mais en général $x\in\mathbb{R}$)
    - mais on ne veut pas biaisé l'attribution d'un nombre à un mot en y introduisant un ordre comme dans le cas où on liste les N mots présents dans les emails auquels on associe un nombre entier de $0$ à $N-1$
    - ainsi on va préférer donner un vecteur de taille $N$ remplis de $0$ sauf pour la position du mot dans la liste de mots
    
Autre exemple: classification de chiffres manuscrites


### Paradigmes d'apprentissage

![](/assets/images/B3.AA.CM.Slide-10.png)

### Exemples Apprentrissage Supervisée

> Keywords: Classification ; Régression

![](/assets/images/B3.AA.CM.Slide-11.png)

### Deep Learning

L'IA est composé de Machine Learning.

Le Deep Leaerning une branche du Machine Learning où les réseaux de neurons sont profond (à plusieurs couches) [peut aussi être sur d'autres niveaux que sur les réseaux de neurones].

On ne sait pas encore à partir de quel seuil un réseau est profond.

![](/assets/images/B3.AA.CM.Slide-12.png)

#### Avant Gout - MLP/DNN

> Keywords:
- FCNN - Fully Connected Neural Network
- DNN - Deep Neural Network

![](/assets/images/B3.AA.CM.Slide-13.png)

Ici, on a un MLP ou aussi appelé FCNN.

La cellule $b$ introduit un biais.

Par exmple, l'image d'un nombre 6 peut être encodé pixel par pixel

Ceci sera linéaire p.r.à. pixel du coup sur la prochaine couche on applique une fonction (la fonction d'activaiton) pour rendre la donnée non linéiare pour gagner en **expressivité**. En effet, ce serait pas intéressé d'avoir une combinaison linéaire des couches en sortir (dans le cas où toutes les couches sont linéaires).

#### Définition

![](/assets/images/B3.AA.CM.Slide-14.png)

#### Origine

![](/assets/images/B3.AA.CM.Slide-15.png)

#### Application

![](/assets/images/B3.AA.CM.Slide-16.png)

##### Image Capturing

![](/assets/images/B3.AA.CM.Slide-17.png)

####

![](/assets/images/B3.AA.CM.Slide-18.png)

#### Segmentation sémantique

![](/assets/images/B3.AA.CM.Slide-19.png)
![](/assets/images/B3.AA.CM.Slide-20.png)

#### Génération d'images

![](/assets/images/B3.AA.CM.Slide-21.png)

Repose sur les "Diffusion Models".
Les photos ici sont créer et non pas choisis dans une base de données.

#### Restons humbles !

![](/assets/images/B3.AA.CM.Slide-22.png)

ChatGPT

### Les fondamentaux

![](/assets/images/B3.AA.CM.Slide-23.png)

#### Le Perceptron

Algorithme de classification

![](/assets/images/B3.AA.CM.Slide-24.png)
![](/assets/images/B3.AA.CM.Slide-25.png)

Ici le perceptron va calculer les points/scores et donner la classe.

![](/assets/images/B3.AA.CM.Slide-26.png)


Il y a autant de $w$ que la taille de $x$ mais un seul seuil/threshold.

Fonction d'activation ???

![](/assets/images/B3.AA.CM.Slide-27.png)

$$
\overrightarrow{w} = [w0 ; w1 ; w2]\\
\overrightarrow{x} = [1 ; x1 ; x2]\\
\overrightarrow{z} = \overrightarrow{w}^t \cdot \overrightarrow{x}
$$

Sur Python, ceci se code:
```python
z = w.T @ x
z = np.dot(w.T, x)
```
Comment trouver $\overrightarrow{w}$ ?

![](/assets/images/B3.AA.CM.Slide-28.png)

Graphiquement on obtient:

![](/assets/images/B3.AA.CM.Slide-29.png)

Ici, $\theta$ représente $\overrightarrow{w}$.

L'équation de la droite rouge est:
$$
\mathcal{D} : w_1x_1 + w_2x_2 + w_0 =  0
$$

Du coup $\overrightarrow{w}$ est perpendiculaire à cette droite.

Cette convergence a été démontré sous l'hypothèse que les données sont séparables (autrement il y aura oscillation).

Du coup, si la droite existe il existera une solution (ou même plusieurs).

Variante équivalentes de la règle:

![](/assets/images/B3.AA.CM.Slide-30.png)

Algorithme:

![](/assets/images/B3.AA.CM.Slide-31.png)

Version Batch:

![](/assets/images/B3.AA.CM.Slide-32.png)

Simuler des fonctions booléennes

![](/assets/images/B3.AA.CM.Slide-33.png)
![](/assets/images/B3.AA.CM.Slide-34.png)

$\hat y$ est la prédiction que l'on compare au label $y$

![](/assets/images/B3.AA.CM.BB20230118-1.png)

![](/assets/images/B3.AA.CM.Slide-35.png)
![](/assets/images/B3.AA.CM.Slide-36.png)

Pour les couches internes sont dites cachés quand on ne sait pas comment reconnaître la vérité (contrairement à la dernière couche).

![](/assets/images/B3.AA.CM.Slide-37.png)
![](/assets/images/B3.AA.CM.Slide-38.png)
![](/assets/images/B3.AA.CM.Slide-39.png)
![](/assets/images/B3.AA.CM.Slide-40.png)
![](/assets/images/B3.AA.CM.Slide-41.png)

Régularisation = conserver des $w$ de norme faible

Meme si avoir une grande norme donne une meilleur confiance

![](/assets/images/B3.AA.CM.Slide-42.png)



> Notes RKA du 2023/01/18 - End

---