---
id: 2rcssepj4fphwhot5h6l2wi
title: CM
desc: ''
updated: 1674494503156
created: 1674053496272
---

> **Avertissement:**
Cette page peut contenir des fautes ! Envoyez-moi un message sur [`#UT3-AURO-M2-2223-Request:matrix.org`](https://matrix.to/#/#UT3-AURO-M2-2223-Request:matrix.org) si vous en trouvez, merci.

> Cours donné par T. Pellegrini

---

> Notes RKA du 2023/01/18 - Start



![](/assets/images/B3.AA.CM.Slide1-001.png)

# Modalités

![](/assets/images/B3.AA.CM.Slide1-002.png)

# Objectifs généraux

![](/assets/images/B3.AA.CM.Slide1-003.png)

Si pas assez de données plustôt utiliser des graphes ou des mélanges de gaussiennes (AI n'est pas toujours le plus optimale).

# Bibiliographie

![](/assets/images/B3.AA.CM.Slide1-004.png)

# Aperçu

> Keywords:
- MLP - Multi-Layer Perceptron
- CNN - Connbolutional Neural Networks
- RNN - Recurrent Neural Networks
- seq2seq

![](/assets/images/B3.AA.CM.Slide1-005.png)

Perceptron est la base d'un réseau de neuronne.

On peut combiner un CNN avec un RNN.

Les modèles séquences à séquences peuvent ne pas être non récurrent.
Les transformeurs sont des MLP sans récurrences.

Limites:
- Interpratibilité: peut-on interpréter le resultat final
- Biais: si les données fournis sont biaisées le resultat sera biaisées aussi donc il faut mettre en place des corrections pour les eviter si possible
- Exemple adversaires: données un modèle erroné pour tester l'algo ???

# Sommaire

![](/assets/images/B3.AA.CM.Slide1-006.png)

# I- Introduction

## Qu'est-ce que l'apprentissage par Machine Learning ?

![](/assets/images/B3.AA.CM.Slide1-007.png)

### Définition

![](/assets/images/B3.AA.CM.Slide1-008.png)

### Terminologie

> Keywords:
- Observations
- Attributs (Features)
- Etiquettes (Labels)
- Corpus (Train) 

![](/assets/images/B3.AA.CM.Slide1-009.png)

Exemple: trier des e-mails
- Données = titres + texte d'email
    - correspondent aux **observations**
- Labels = Binaire --> Spam/Non-Spam
    - y = +1 pour Spam
    - y = 0  pour Non-Spam (**dans un perceptron on mettre à -1**)
- Comment encoder le texte ?
    - on cherche à travailler avec des réelles (il existe des algo binaires mais en général $x\in\mathbb{R}$)
    - mais on ne veut pas biaisé l'attribution d'un nombre à un mot en y introduisant un ordre comme dans le cas où on liste les N mots présents dans les emails auquels on associe un nombre entier de $0$ à $N-1$
    - ainsi on va préférer donner un vecteur de taille $N$ remplis de $0$ sauf pour la position du mot dans la liste de mots
    
Autre exemple: classification de chiffres manuscrites


### Paradigmes d'apprentissage

![](/assets/images/B3.AA.CM.Slide1-010.png)

### Exemples Apprentrissage Supervisée

> Keywords: Classification ; Régression

![](/assets/images/B3.AA.CM.Slide1-011.png)

## Deep Learning

L'IA est composé de Machine Learning.

Le Deep Leaerning une branche du Machine Learning où les réseaux de neurons sont profond (à plusieurs couches) [peut aussi être sur d'autres niveaux que sur les réseaux de neurones].

On ne sait pas encore à partir de quel seuil un réseau est profond.

> Keywords: niveaux d'abstraction

![](/assets/images/B3.AA.CM.Slide1-012.png)

### Avant Gout - MLP/DNN

> Keywords:
- MLP - Multi-Layer Perceptron
- FCNN - Fully Connected Neural Network
- DNN - Deep Neural Network

![](/assets/images/B3.AA.CM.Slide1-013.png)

Ici, on a un MLP ou aussi appelé FCNN.

La cellule $b$ introduit un biais.

Par exmple, l'image d'un nombre 6 peut être encodé pixel par pixel

Ceci sera linéaire p.r.à. pixel du coup sur la prochaine couche on applique une fonction (la fonction d'activaiton) pour rendre la donnée non linéiare pour gagner en **expressivité**. En effet, ce serait pas intéressé d'avoir une combinaison linéaire des couches en sortir (dans le cas où toutes les couches sont linéaires).

### Définition

> Keywords:
- Représentation distribuée (distributed representation)
- Neurones activés
- Partionnement de l'entrée

![](/assets/images/B3.AA.CM.Slide1-014.png)

### Origine

![](/assets/images/B3.AA.CM.Slide1-015.png)

## Application

![](/assets/images/B3.AA.CM.Slide1-016.png)

### Image Captioning

![](/assets/images/B3.AA.CM.Slide1-017.png)
![](/assets/images/B3.AA.CM.Slide1-018.png)

### Segmentation sémantique

![](/assets/images/B3.AA.CM.Slide1-019.png)
![](/assets/images/B3.AA.CM.Slide1-020.png)

### Génération d'images

![](/assets/images/B3.AA.CM.Slide1-021.png)

Repose sur les "Diffusion Models".
Les photos ici sont créer et non pas choisis dans une base de données.

### Remarques

![](/assets/images/B3.AA.CM.Slide1-022.png)

ChatGPT

# II- Les fondamentaux

![](/assets/images/B3.AA.CM.Slide1-023.png)

## Le Perceptron

Algorithme de classification

![](/assets/images/B3.AA.CM.Slide1-024.png)
![](/assets/images/B3.AA.CM.Slide1-025.png)

Ici le perceptron va calculer les points/scores et donner la classe.

![](/assets/images/B3.AA.CM.Slide1-026.png)


Il y a autant de $w$ que la taille de $x$ mais un seul seuil/threshold.

![](/assets/images/B3.AA.CM.Slide1-027.png)

$$
\overrightarrow{w} = [w0 ; w1 ; w2]\\
\overrightarrow{x} = [1 ; x1 ; x2]\\
\overrightarrow{z} = \overrightarrow{w}^t \cdot \overrightarrow{x}
$$

Sur Python, ceci se code:
```python
z = w.T @ x
z = np.dot(w.T, x)
```
## Comment trouver $\overrightarrow{w}$ ?

![](/assets/images/B3.AA.CM.Slide1-030.png)

Graphiquement on obtient:

![](/assets/images/B3.AA.CM.Slide1-031.png)

Ici, $\theta$ représente $\overrightarrow{w}$.

L'équation de la droite rouge est:
$$
\mathcal{D} : w_1x_1 + w_2x_2 + w_0 =  0
$$

Du coup $\overrightarrow{w}$ est perpendiculaire à cette droite.

Cette convergence a été démontré sous l'hypothèse que les données sont séparables (autrement il y aura oscillation).

Du coup, si la droite existe il existera une solution (ou même plusieurs).

## Variante équivalentes de la règle

![](/assets/images/B3.AA.CM.Slide1-032.png)

## Algorithme

![](/assets/images/B3.AA.CM.Slide1-034.png)

## Version Batch

![](/assets/images/B3.AA.CM.Slide1-035.png)

## Simuler des fonctions booléennes

![](/assets/images/B3.AA.CM.Slide1-036.png)
![](/assets/images/B3.AA.CM.Slide1-037.png)
![](/assets/images/B3.AA.CM.BB20230118-1.png)

$\hat y$ est la prédiction que l'on compare au label $y$

# III- Du Perceptron au MLP

![](/assets/images/B3.AA.CM.Slide1-038.png)

## MLP - Multi Layer Perceptron

![](/assets/images/B3.AA.CM.Slide1-039.png)

Pour les couches internes sont dites cachés quand on ne sait pas comment reconnaître la vérité (contrairement à la dernière couche).

## Frontières de décision

### Exemple: Un polygone

![](/assets/images/B3.AA.CM.Slide1-040.png)
![](/assets/images/B3.AA.CM.Slide1-046.png)


Régularisation = conserver des $w$ de norme faible

Meme si avoir une grande norme donne une meilleur confiance

### Exemple: Deux polygones

![](/assets/images/B3.AA.CM.Slide1-047.png)

## Problématique

![](/assets/images/B3.AA.CM.Slide1-048.png)

## Classification linéaire: cas multiclasse

![](/assets/images/B3.AA.CM.Slide1-049.png)

### Formule

![](/assets/images/B3.AA.CM.Slide1-050.png)




> Notes RKA du 2023/01/18 - End

---

> Notes RKA du 2023/01/19 - Start




En cas multi-class on prédit une seule classe gagnante. Du coup la classe prédite sera celle avec le meilleur score.

Sur Python, on code cela ainsi:

```python
# how to return the max value in scores
np.max(scores)
y_pred = np.argmax(scores) # returns the index of the max value in the vector scores

# other option
import torch
y_pred = torch.argmax(scores)
```

$W$ est de taille $k\times d$ et $x$ de taille $d$ et $scores$ est de taille $k$

### Exemple

![](/assets/images/B3.AA.CM.Slide1-051.png)


### CIFAR-10

![](/assets/images/B3.AA.CM.Slide1-052.png)

On peut faire un redimensionnement de W pour l'afficher avec `imshow` ce qui donne les images dans le slide

```python
import matplotlib.pyplot as plt

plt.imshow(W_fox_reshaped)
```

Ici on a des images de taille 32x32.

Comme il y a de la couleur on a un tenseur de dimension 32x32x3.

Du coup x est une superposition de 3 vecteurs 1024.

On peut interpréter l'image de la classe avion comme un filtre qui donnera un score elevé dès que l'image candidate possède un fond bleu.

Pour le cheval le filtre ne fait pas de différence si la tete du cheval est à gauche ou à droite.

### Remarque technique

![](/assets/images/B3.AA.CM.Slide1-053.png)
![](/assets/images/B3.AA.CM.Slide1-054.png)

### Comment trouver $\overrightarrow{w}$ ?

![](/assets/images/B3.AA.CM.Slide1-056.png)
![](/assets/images/B3.AA.CM.Slide1-058.png)

#### La descente du gradient (DG)

> Keywords:
- $\alpha$: taux d'apprentissage

![](/assets/images/B3.AA.CM.Slide1-059.png)

### Quelle fonction _loss_ choisir ?

![](/assets/images/B3.AA.CM.Slide1-060.png)

#### Heaviside 

![](/assets/images/B3.AA.CM.Slide1-062.png)

- si $y\times w^t\times x$ est negatif on fait une erreur (comme `heaviside` est en logique inverse il faut le moins pour obtenir le bon _loss_)

#### Choix du _loss_

![](/assets/images/B3.AA.CM.Slide1-063.png)

##### Activation: Sigmoide

> La fonction Sigmoide est aussi appelé fonction logistique

![](/assets/images/B3.AA.CM.Slide1-064.png)
![](/assets/images/B3.AA.CM.Slide1-065.png)
![](/assets/images/B3.AA.CM.Slide1-066.png)

Historiquement la sigmoide a été utilisé c'est pourquoi les labels d'un perceptron étaient 0 ou +1 (on aurait tangente hyperbolique qui donne -1 +1 mais la sigmoide a été utilisé).

Maintenant une autre fonction est utilisé que l'on verra plus tard.

##### Critère de choix

![](/assets/images/B3.AA.CM.Slide1-068.png)

##### BCE - Binary Cross-Entropy

![](/assets/images/B3.AA.CM.Slide1-069.png)

Dans le cas $y=1$ la loss pénalise lorsque le score est proche de 0 via sa divergence en 0.

##### Régression logistique (ou classification logistique)

![](/assets/images/B3.AA.CM.Slide1-070.png)
![](/assets/images/B3.AA.CM.Slide1-071.png)

```python
import torch
import torch.nn as nn

class LogisticRegression(nn.Module):
    # Constructor
    def __init__(self, input_dim):
        super(LogiticRegression, self).__init__()
        # we need a linear layer with weight of input_dim and a bias of 1
        self.linear = torch.nn.Linear(input_dim, 1)
        # if need several layers we have to define a self.linear2

    # Method
    def forward(self, x): # forward is reserved this is why we do not need to write .forward when calling it
        # We apply the function linear defined in the constructor
        a = self.linear(x)
        # with several layers we would code:
        # x = self.linear(x)
        # We use sigmoid to compute our prediction
        y_pred = torch.sigmoid(a)
        return y_pred

# Object Creation of input_dim = 10
model = LogiticRegression(10)

# Random Matrix of size 100x10 filled with numbers center in 0 with variance -1:1
input = torch.randn(100, 10) # this is a tensor
output = model(input) # works better (regarding memory's management) than model.forward(input)
print(output.size())
```


##### Fonctions d'activation $g$ et $o$

![](/assets/images/B3.AA.CM.Slide1-073.png)

Le choix de la fonction d'activaiton peut se trouver dans la doc:
- [TORCH.NN](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)

> Fonctions: tanh ; hardthanh ; sigmoid ; softplus ; relu ; leakyrelu
![](/assets/images/B3.AA.CM.Slide1-074.png)

Sigmoide n'est plus utilisé car lorsque la fonction est 'platte' la dérivée est nulle ce qui signifie que le gradient "s'evapore" et donc ne fait plus évoluer le W du coup le réseau n'apprend plus.

Relu est facile à coder `max(0,x)`

Mais de nos jours la plus utilisé est `nn.GELU` (cf [doc](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU))

![](/assets/images/B3.AA.CM.Slide1-074.GELU.png)

##### Softmax

![](/assets/images/B3.AA.CM.Slide1-075.png)

![](/assets/images/B3.AA.CM.BB20230119-1.png)

$a = w^t \times x$ est aussi appelé la préactivation

L'activation est noté $h$

![](/assets/images/B3.AA.CM.BB20230119-2.png)

La couche de sortie présente des resultats probabiliste.

## Apprentissage 

### Cas: MLP

![](/assets/images/B3.AA.CM.Slide1-079.png)

### Cas: réseau de neuronne

![](/assets/images/B3.AA.CM.Slide1-080.png)

`forward` passe les exemples dans le model

`backword` calcule le gradient de la _loss_ pour chaque couches en partant de la sortie

une fois le réseau entrainé il n'y a plus besoin de faire un `backward`

### Algo

![](/assets/images/B3.AA.CM.Slide1-085.png)

## Classification

### Quelle _loss_ ?

![](/assets/images/B3.AA.CM.Slide1-086.png)
![](/assets/images/B3.AA.CM.Slide1-087.png)

La _loss_ (cf [doc](https://pytorch.org/docs/stable/nn.html#loss-functions) ) n'est pas à confondre avec la fonction d'activation ! La fct d'activaiton s'applique dans les couches (et peut être différent à chaque couche) elle prend pour valeur les sorties des couches. La _loss_ elle prend les labels et les sorties pour mesuser l'erreur.

La _cross-entropy loss_ se définit comme suit:
$$
\mathcal{L}(y,\hat y) = -log(\hat y_c) = \mathbb{I}\{ i = c \} \overrightarrow{y} \; log \overrightarrow{\hat y} 
$$


> Notes RKA du 2023/01/19 - End

---