---
id: ueu7w9ena2kxjuhtka1v7cx
title: Filtre Particulaire
desc: ''
updated: 1674031678510
created: 1673938245760
---

> **Avertissement:**
Cette page peut contenir des fautes ! Envoyez-moi un message sur [`#UT3-AURO-M2-2223-Request:matrix.org`](https://matrix.to/#/#UT3-AURO-M2-2223-Request:matrix.org) si vous en trouvez, merci.

> Cours donné par P. Danès

Support de cours:
- BlackBoard:
    - [B3.RP.CM.BB20230117.pdf](https://github.com/TunnARK/UT3-AURO-2223-S10-Dendron/blob/main/vault/assets/B3.RP.CM.BB20230117.pdf)
    - [B3.RP.CM.BB20230118.pdf](https://github.com/TunnARK/UT3-AURO-2223-S10-Dendron/blob/main/vault/assets/B3.RP.CM.BB20230118.pdf)
- Notes GEA:
    - [B3.RP.CM.NotesGEA.20230117.pdf](https://github.com/TunnARK/UT3-AURO-2223-S10-Dendron/blob/main/vault/assets/B3.RP.CM.NotesGEA.20230117.pdf)
    - [B3.RP.CM.NotesGEA.20230118.pdf](https://github.com/TunnARK/UT3-AURO-2223-S10-Dendron/blob/main/vault/assets/B3.RP.CM.NotesGEA.20230118.pdf)

---

> Notes RKA du 2023/01/17 - Start





![](/assets/images/B3.RP.CM.BB20230117-01.png)

- Sequential Monte Carlo methods
- Particle filtering

# Pourquoi ?

- Très utilisé pour le filtrage Bayésien NL non Gaussien
- Existence d'une solution emblématique du SLAM (algo FAST SLAM / "gmapping")

# I- Rappels

## 1) Filtrage Bayésien

![](/assets/images/B3.RP.CM.BB20230117-02.png)

<!-- graphe x_0 -->

- séquence d'**états cachés** $x_{0:k} = (x_0, \dots, x_k)$
- séquence d'**observations** $z_{1:k} = (z_1, \dots, z_k)$

### Contexte incertain (approche probabiliste)

![](/assets/images/B3.RP.CM.BB20230117-03.png)

- $x_{0:k}$ réalisation de $X_{0:k}$ processus aléatoire d'**état** (séquence de variable aléatoire $X_0$, $\dots$, $X_k$)
- $x_{0:k} = X_{0:k}(\omega)$ résultat de l'expérience en cours / **caché**
- $z_{0:k}$ réalisation de $Z_{0:k}$ processus aléatoire de **mesure**
    - $z_{0:k} = Z_{0:k}(\omega)$ résultat de l'expérience en cours / **accessible**

### Connaissance a priori

Description de la **connaissance a priori** en termes probabilistes

- P.A. d'état caché Markonien
    - loi de $X_{0:k}$ décrite par
        - loi de $X_0$ (**loi initiale**)
        - loi statistique entre $X_{k-1}$ et $X_k$ (**loi/modèle de dynamique a priori**)
- P.A. de mesure
    - loi entre $X_k$ caché et $Z_k$ la **loi/modèle d'observation**

### Objectif: Loi A Posteriori/Postérieure

$$
P_{X_{0:k} \,|\, z_{1:k}}\big( x_{0:k} \,|\, z_{1:k} \big)\\
\ \\\text{ou}\\ \ \\
P_{X_{k} \,|\, z_{1:k}}\big( x_{k} \,|\, z_{1:k} \big)\\
$$

![](/assets/images/B3.RP.CM.BB20230117-04.png)

<!-- 
## Discussion

image graph + audio 1 
-->

### Exemple: Loi Postérieure

![](/assets/images/B3.RP.CM.BB20230117-05.png)

<!-- audio 2 -->

Imaginons que l'on sache trier les expériences qui donne pair, ceci nous permettrait d'avoir un modèle d'observation.

Idem pour les impairs, en calculant $\mathbb{P}(Z=z \,|\, X = 2)$ on peut en déduire un modèle d'observation.

C'est à dire qu'ici on décrit notre connaissance a priori.

Maintenant si on observe du vert quel est l'etat caché (le numéro sous la couleur verte) ?

On ne sait pas la valeur exacte mais on est sûr que cette valeur peut être 1, 3 ou 5.

Ceci est un problème d'estimation.

<!-- Audio 3 -->

On peut faire de même pour la couleur rouge et bleu (avec la différence particulière que pour le bleu on connait l'état avec certitude).

Dans notre contexte, on va travailler sur un continium et non plus une "grille"/dans du discret. Ceci va nous amener à introduire la notion de densité probabliste.

### MMSE - Estimé

> MMSE = Minimum Mean Square (Error) Estimate

![](/assets/images/B3.RP.CM.BB20230117-06.png)

$$
\mathbb{E} \big[ X_k \,|\, Z_{1:k}=z_{1:k} \big] = \int x_k \, P_{X_k \,|\, Z_{1:k}} \big( x_k \,|\, z_{1:k} \big) \, dx_k
$$

<!-- audio 4 -->

### Densité de probabilité

#### Continium

![](/assets/images/B3.RP.CM.BB20230117-07.png)

Soit $x \in \mathbb{R}$, $P_X(x)$ est une densité de probabilité (dont la variable est $x$ et $X$ sa réalisation possible) tel que :
- $P_X(x) \geq 0$
- $\int_{\mathbb{R}} P_X(x)\, dx = \mathbb{P}(x\in\mathbb{R}) = 1$
- $\int_{\epsilon} P_X(x)\, dx = \mathbb{P}(x\in\epsilon)$

<!-- audio 5 -->

![](/assets/images/B3.RP.CM.BB20230117-08.png)

Quelle est l'**espérance de X** ? Est-ce la valeur moyenne des issues possibles de $X$ ?

$$
\mathbb{E}\big[X\big] = \mathbb{E}_{P_X(x)}\big[X\big] = \int_{\mathbb{R}} x\,P_X(x)\,dx = \bar x
$$

Comment se disperse $X$ autour de $\mathbb{E}\big[X\big]$ ?

$$
Var\big[X\big] = \mathbb{E}_{P_X(x)}\Big[ \big( X - \mathbb{E}[X] \big)^2 \Big] = \mathbb{E}\big[ X^2 \big] - \Big(\mathbb{E}\big[ X \big]\Big)^2
$$

#### Discret

Si maintenant $x\in\mathbb{R}$ est à valeurs discrètes.

![](/assets/images/B3.RP.CM.BB20230117-09.png)


<!-- audio 6
implusion de dirac pour passer du continu au discret -->

![](/assets/images/B3.RP.CM.BB20230117-10.png)


<!-- audio 7 -->


$$
\int_{\mathbb{R}} f(x)\,\delta(x-x_i)\,dx = f(x_i)\\
$$

Convolution d'une impulsion de dirac
$$
\int_{\mathbb{R}} f(x)\,\delta(x-x_i)\,dx = f(x-x_i)\\
$$

#### Multi-variable

![](/assets/images/B3.RP.CM.BB20230117-11.png)
![](/assets/images/B3.RP.CM.BB20230117-12.png)
![](/assets/images/B3.RP.CM.BB20230117-13.png)
<!-- audio 8 -->

## 2) Lois Gaussiennes

<!-- audio 9 -->

### Mono-variable

![](/assets/images/B3.RP.CM.BB20230117-14.png)

- $m\pm\sigma$ correspond au point de flexion de la cloche gaussienne

![](/assets/images/B3.RP.CM.BB20230117-15.png)

### Multi-variable

![](/assets/images/B3.RP.CM.BB20230117-16.png)



> Notes RKA du 2023/01/17 - End

---

> Notes RKA du 2023/01/18 - Start



### Comment échantillonner une loi Gaussienne multivariée ?

> i.i.d. Indépendant Identiquement Distribué

Se pose deux problèmes :
- Comment tirer une colonne avec les variables voulues (e.g. `Y = randm(M,N)` )
- Trouver le bon $L$ tq $LL^T = P$

![](/assets/images/B3.RP.CM.BB20230118-01.png)
![](/assets/images/B3.RP.CM.BB20230118-02.png)

### Nota: Estimation Bayésienne

##### Loi A Posteriori

![](/assets/images/B3.RP.CM.BB20230118-03.png)
![](/assets/images/B3.RP.CM.BB20230118-04.png)
![](/assets/images/B3.RP.CM.BB20230118-05.png)

L'estimateur $\hat X_{MMSE}$ est indépendant de la métrique utilisé (i.e. $Q$ ) du coup il n'est pas possible d'affiner l'estimateur pour être plus précis sur un bloc de variable et pas un autre. On estime le tout d'office.

##### Covariance A Posteriori

![](/assets/images/B3.RP.CM.BB20230118-06.png)

### Compléments Estimation Dynamique

> Attention: pour simplifier les notations on ne va plus distinguer entre une var. al. et sa réalisation !!! 

> Sauf contre-indication tous les bruits seront considérés blanc (i.e. "_indépendance des echantillons_")

> Attention: $\quad P_0 \geq 0 \quad\wedge\quad Q_{k-1} \geq 0 \quad\wedge\quad W_{0:k}\;\;blanc$

> Trois cas:
- $k'\leq k$ alors **prédiction**
- $k'= k$ alors **filtrage**
- $k'\geq k$ alors **lissage**

![](/assets/images/B3.RP.CM.BB20230118-07.png)
![](/assets/images/B3.RP.CM.BB20230118-08.png)
![](/assets/images/B3.RP.CM.BB20230118-09.png)
![](/assets/images/B3.RP.CM.BB20230118-10.png)
![](/assets/images/B3.RP.CM.BB20230118-11.png)
![](/assets/images/B3.RP.CM.BB20230118-12.png)
![](/assets/images/B3.RP.CM.BB20230118-13.png)

# II- Approximation (ponctuelle) de Monte Carlo (d'une pdf)

![](/assets/images/B3.RP.CM.BB20230118-14.png)



> Notes RKA du 2023/11/18 - End

---